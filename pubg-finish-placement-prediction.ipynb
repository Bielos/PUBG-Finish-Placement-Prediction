{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15b37adb9992540b453c754f9fde24b21d780dd2"
   },
   "source": [
    "# PUBG Finish Placement Prediction\n",
    "\n",
    "Autor: Daniel Martinez Bielostotzky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d8fd3394d1fd1798c8270fe2e32c7cfff552db2"
   },
   "source": [
    "## Table of contents\n",
    "* **Imports: Dataset, Libraries and Usefull Functions**\n",
    "* **Preprocessing: Missing Values**\n",
    "* **Feature Engenieering: Team and Match Features**\n",
    "* **Feature Selection and Outliers**\n",
    "* **LightGBM Model**\n",
    "* **Test Data Prediction and Submit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d586e88e4c6def12c2ba949f076a5dc8bdf20bcd"
   },
   "source": [
    "## Imports: Dataset, Libraries and Usefull Functions\n",
    "\n",
    "For this notebook, I'll use two functions that are from a kind of EDA framework that I always use and that its open to contributions on [GitHub](https://github.com/Bielos/EDA-Framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "def get_null_observations(dataframe, column):\n",
    "    return dataframe[pd.isnull(dataframe[column])]\n",
    "\n",
    "def delete_null_observations(dataframe, column):\n",
    "    fixed_df = dataframe.drop(get_null_observations(dataframe,column).index)\n",
    "    return fixed_df\n",
    "    \n",
    "def get_missing_data_table(dataframe):\n",
    "    total = dataframe.isnull().sum()\n",
    "    percentage = dataframe.isnull().sum() / dataframe.isnull().count()\n",
    "    \n",
    "    missing_data = pd.concat([total, percentage], axis='columns', keys=['TOTAL','PERCENTAGE'])\n",
    "    return missing_data.sort_index(ascending=True)\n",
    "\n",
    "df = pd.read_csv('../input/train_V2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d941ddafaa45768a44556ad97c1c989216bb2e6b"
   },
   "source": [
    "## Preprocessing: Missing Values\n",
    "\n",
    "To check the integrity of the data, the missing values and data types are displayed.\n",
    "### Missing Vales\n",
    "Using *get_missing_data_table* we can see that the training dataset has only one record with a missing value in the 'winPlacePerc' column since it is the target column no completion method can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10221cf930a3a3de7fb8c39f47299151b4f579a4"
   },
   "outputs": [],
   "source": [
    "get_missing_data_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e65338e87a5f71bbfe377977fa4fc8fe306c36b1"
   },
   "outputs": [],
   "source": [
    "df = delete_null_observations(dataframe=df, column='winPlacePerc')\n",
    "get_missing_data_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "246de470658de11a66fb340d01e2af582e8db2b1"
   },
   "source": [
    "## Feature Engenieering: Team and Match Features\n",
    "Grouping records by *groupId* and *matchId* the features *teamKills* (Sum of kills in the team), *teamSize* (Total number of players in the team), *matchKills* and *matchSize* are created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c779db5b45eb7a9c0f3fa791da1b1eec8fbeca6f"
   },
   "outputs": [],
   "source": [
    "# Adding team features\n",
    "df_team_dict = (df.groupby('groupId', as_index = True)\n",
    "          .agg({'Id':'count', 'kills':'sum'})\n",
    "          .rename(columns={'Id':'teamSize', 'kills':'teamKills'})).to_dict()\n",
    "\n",
    "teamKills = []\n",
    "teamSize = []\n",
    "\n",
    "for teamId in df['groupId']:\n",
    "    teamKills.append(df_team_dict['teamKills'][teamId])\n",
    "    teamSize.append(df_team_dict['teamSize'][teamId])\n",
    "\n",
    "df['teamKills'] = teamKills\n",
    "df['teamSize'] = teamSize\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc35f9c1c9e91b1e07ff2b43a468c6256d7058b2"
   },
   "outputs": [],
   "source": [
    "# Adding match features\n",
    "df_team = (df.groupby('groupId', as_index = False)\n",
    "          .agg({'Id':'count', 'matchId':lambda x: x.unique()[0], 'kills':'sum'})\n",
    "          .rename(columns={'Id':'teamSize', 'kills':'teamKills'})).reset_index()\n",
    "\n",
    "df_match = (df_team.groupby('matchId', as_index = True)\n",
    "           .agg({'teamSize':'sum', 'teamKills':'sum'})\n",
    "           .rename(columns={'teamSize':'matchSize', 'teamKills':'matchKills'})).to_dict()\n",
    "matchSize = []\n",
    "matchKills = []\n",
    "\n",
    "for matchId in df['matchId']:\n",
    "    matchSize.append(df_match['matchSize'][matchId])\n",
    "    matchKills.append(df_match['matchKills'][matchId])\n",
    "\n",
    "df['matchSize'] = matchSize\n",
    "df['matchKills'] = matchKills\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cdc8ac94573aad5d26532147a2e63cdf0919879"
   },
   "source": [
    "## Feature Selection and Outliers\n",
    "\n",
    "Features that represent IDs are meaningless for any model so they are dropped out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcf492bdfd7d79e0832f86943044bf7fc2c49dcf"
   },
   "outputs": [],
   "source": [
    "#Drop insignificant features\n",
    "df.drop(['Id'], axis='columns', inplace=True)\n",
    "df.drop(['groupId'], axis='columns', inplace=True)\n",
    "df.drop(['matchId'], axis='columns', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3ff353a0f44e53d431d88b132bc1e8334db6fc3"
   },
   "source": [
    "### Outliers\n",
    "Some records may be rare cases and may affect the generalization power of the model because they are just noise.\n",
    "\n",
    "The outliers to be deleted are:\n",
    "1.  Records with low *matchDuration* (According to box plot) \n",
    "1. Players with 0 *rideDistance* and *roadKills* greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06dc0cd06d7b26220a9d70460f5c7fa452697893"
   },
   "outputs": [],
   "source": [
    "# matchDuration boxplot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sn.boxplot(data=df['matchDuration'], ax= ax)\n",
    "ax.set(title='Match Duration Box Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa7f72fa0a8d6fcda9ab2b453ad0e5f482e7499a"
   },
   "outputs": [],
   "source": [
    "# Delete Outliers according to matchDuration\n",
    "previous_record_size = df.shape[0]\n",
    "\n",
    "h_spread = df['matchDuration'].quantile(.75) - df['matchDuration'].quantile(.25)\n",
    "limit = df['matchDuration'].quantile(.25) - 2 * h_spread\n",
    "df.drop(df[df['matchDuration'] < limit].index, inplace=True)\n",
    "\n",
    "new_record_size = df.shape[0]\n",
    "print('Total records deleted: {} ({:.7%} of previous record size)'.format(previous_record_size - new_record_size, 1 - new_record_size / previous_record_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fc0e0de167d3f463ec947d495c3f567431b8f91"
   },
   "outputs": [],
   "source": [
    "# Delete Outliers according to rideDistance and roadKills\n",
    "previous_record_size = df.shape[0]\n",
    "\n",
    "df.drop(df.query('rideDistance == 0 and roadKills > 0').index, inplace=True)\n",
    "\n",
    "new_record_size = df.shape[0]\n",
    "print('Total records deleted: {} ({:.7%} of previous record size)'.format(previous_record_size - new_record_size, 1 - new_record_size / previous_record_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1940bac7b544b5ce3e647365230f0b6675639e89"
   },
   "source": [
    "## LightGBM Model\n",
    "\n",
    "A LightGBM model is used to predict the target *winPlacePerc*,  the model use 15000 iterations, 70% of features and 90% of training data per tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6771134cecf32fa7702083de62d61db798553a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label encode matchType\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df['matchType'] = encoder.fit_transform(df['matchType'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f2e59e593602b3f112570f1a578f05234968973"
   },
   "outputs": [],
   "source": [
    "# X and y split\n",
    "y = df['winPlacePerc'].values\n",
    "X = df.drop(['winPlacePerc'], axis='columns').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a43904e9068788357cab50227ad55924430f53b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=[12])\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# set matchType\n",
    "\n",
    "params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"mae\",\n",
    "        \"n_estimators\":15000,\n",
    "        \"early_stopping_rounds\":100,\n",
    "        \"num_leaves\" : 31, \n",
    "        \"learning_rate\" : 0.05, \n",
    "        \"bagging_fraction\" : 0.9,\n",
    "        \"bagging_seed\" : 0, \n",
    "        \"num_threads\" : 4,\n",
    "        \"colsample_bytree\" : 0.7\n",
    "        }\n",
    "\n",
    "model = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5,\n",
    "                verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d41e2c846c19af385bb25cfcc047bf1ac0ac444d"
   },
   "source": [
    "## Test Data Prediction and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d6f1251f487de6e775ead296a5205dc1d8ac761"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/test_V2.csv')\n",
    "df_test['matchType'] = encoder.transform(df_test['matchType'])\n",
    "df_test_team_dict = (df_test.groupby('groupId', as_index = True)\n",
    "          .agg({'Id':'count', 'kills':'sum'})\n",
    "          .rename(columns={'Id':'teamSize', 'kills':'teamKills'})).to_dict()\n",
    "\n",
    "teamKills_test = []\n",
    "teamSize_test = []\n",
    "\n",
    "for teamId in df_test['groupId']:\n",
    "    teamKills_test.append(df_test_team_dict['teamKills'][teamId])\n",
    "    teamSize_test.append(df_test_team_dict['teamSize'][teamId])\n",
    "\n",
    "df_test['teamKills'] = teamKills_test\n",
    "df_test['teamSize'] = teamSize_test\n",
    "\n",
    "df_team_test = (df_test.groupby('groupId', as_index = False)\n",
    "          .agg({'Id':'count', 'matchId':lambda x: x.unique()[0], 'kills':'sum'})\n",
    "          .rename(columns={'Id':'teamSize', 'kills':'teamKills'})).reset_index()\n",
    "\n",
    "df_match_test = (df_team_test.groupby('matchId', as_index = True)\n",
    "           .agg({'teamSize':'sum', 'teamKills':'sum'})\n",
    "           .rename(columns={'teamSize':'matchSize', 'teamKills':'matchKills'})).to_dict()\n",
    "matchSize_test = []\n",
    "matchKills_test = []\n",
    "\n",
    "for matchId in df_test['matchId']:\n",
    "    matchSize_test.append(df_match_test['matchSize'][matchId])\n",
    "    matchKills_test.append(df_match_test['matchKills'][matchId])\n",
    "\n",
    "df_test['matchSize'] = matchSize_test\n",
    "df_test['matchKills'] = matchKills_test\n",
    "\n",
    "X_testdata = df_test.drop(['Id','groupId','matchId'], axis='columns').values\n",
    "\n",
    "df_test['winPlacePerc'] = model.predict(X_testdata, num_iteration=model.best_iteration)\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
